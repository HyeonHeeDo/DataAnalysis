{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 토큰화\n",
    ": 주어진 코퍼스corpus에서 토큰token이라 불리는 단위로 나누는 작업\n",
    "\n",
    "## 1. 단어 토큰화\n",
    ": 단어 기준으로 토큰화  \n",
    "* 단어 : 단어 + 단어구, 의미를 갖는 문자열\n",
    "  \n",
    "  \n",
    "예)  \n",
    "input : Time is an illusion. Lunchtime double so!  \n",
    "output: \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"\n",
    "\n",
    "## 2. 토큰화 중 생기는 선택의 순간\n",
    ": 토큰화의 기준 생각\n",
    "* NLTK : 영어 코퍼스 토큰화 위한 도구 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# wordPunkTokenizer\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# using keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 토큰화에서 고려 사항\n",
    "1. 구두점 / 특수문자 단순 제외하면 안됨\n",
    "2. 줄임말 / 단어 내에 띄어쓰기 있는 경우 : 사용 용도에 따라 하나의 토큰으로 봐야할 수도\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준 토큰화 예제: Penn Treebank Tokenization 규칙\n",
    "1. 하이픈으로 구성된 단어는 하나로 유지\n",
    "2. apostrophe로 생기는 줄입말(접어) 분리\n",
    "\n",
    "__input sentence :__  \n",
    "\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문장 토큰화\n",
    ": 토큰의 단위가 문장일 때  \n",
    ": 갖고있는 코퍼스 내에서 문장 단위로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "# 영어 문장 토큰화\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "# 온점이 여러번 등장하는 문장\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 이진 분류기\n",
    ": 예외 사항 발생시키는 온점의 처리를 위해 입력에 따라 2개의 클래스로 분류하는 이진 분류기 사용\n",
    "1. 온점이 단어의 일부분인 경우 (온점이 약어로 사용)\n",
    "2. 온점이 정말 문장의 구분자인 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 한국어의 토큰화 어려움\n",
    "* 어절(띄어쓰기 기준) 토큰화 ≠ 단어 토큰화 → 형태소 토큰화 실행\n",
    "* 띄어쓰기 어렵고 잘 안지켜짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 품사 태깅\n",
    ": 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NLTK 이용 - 영어 토큰화 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x = word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "# 2) 정제 cleaning & 정규화normalization\n",
    "* 토큰화 : 코퍼스에서 용도에 맞게 토큰 분류 - 전/후에 정제 및 정규화\n",
    "* 정제 :  갖고 있는 코퍼스로부터 노이즈 제거\n",
    "* 정규화: 표현 방법이 다른 단어 통합, 같은 단어로 만든다 (복잡성 줄이기)\n",
    "    \n",
    "    \n",
    "    \n",
    "## 1. 표기가 다른 단어들을 하나의 단어로 정규화\n",
    "\n",
    "\n",
    "## 2. 대소문자 통합 (소문자 변환)\n",
    "* 구분되어야 하는 경우도 존재\n",
    "* 일부만 소문자로 변환\n",
    "    \n",
    "    \n",
    "## 3. 불필요한 단어 제거\n",
    "* 노이즈 데이터\n",
    "    * 자연어가 아니면서 아무 의미 없는 글자 (예: 특수문자)\n",
    "    * 분석하고자 하는 목적에 맞지 않는 불필요한 단어\n",
    "\n",
    "### (1) 등장 빈도 적은 단어\n",
    "\n",
    "### (2) 길이가 짧은 단어 (영어권 언어)\n",
    "* 의미 없는 단어 제거 효과\n",
    "* 짧은 단어 삭제하면 구두점들까지도 한번에 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2 인 단어들을 정규 표현식을 이용하여 삭제\n",
    "import re\n",
    "\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "# \\b : 단어의 경계 (공백,탭, ',', '/' 등)\n",
    "# \\w : 단어를 만들 수 있는 글자 (알파멧 대소문자, 숫자, _ 등)\n",
    "# \\W : not \\w\n",
    "\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 정규 표현식\n",
    "* 코퍼스에서의 노이즈 데이터 특징 찾은 후 정규 표현식 통해 제거\n",
    "  \n",
    "\n",
    "***\n",
    "\n",
    "  \n",
    "# 3) 어간 추출 stemming & 표제어 추출 lemmatization\n",
    ": 코퍼스의 단어 갯수 줄일 수 있는 정규화 기법  \n",
    ": 서로 다른 단어를 하나의 단어로 일반화시켜 문서 내 단어 수 줄이기  \n",
    ": 단어 빈도수 기반 BoW (Bag of Words) 표현 사용하는 자연어 처리 문제에 사용\n",
    "  \n",
    "  \n",
    "* 표제어 추출: 문맥 고려, 수행 결과는 해당 단어의 폼사 정보 보존\n",
    "* 어간 추출 : 수행 결과는 품사 정보 보존하지 않음 \n",
    "\n",
    "## 1. 표제어 추출\n",
    "* lemma : 표제어. 기본 사전형 단어\n",
    "* 표제어 추출 : 단어들로부터 표제어 찾기 = 다른 형태 단어들의 갯수 줄이기\n",
    "    * 형태학적 파싱\n",
    "        * 형태소 : 의미를 가진 가장 작은 단위\n",
    "            1. 어간 stem : 단어의 의미를 지닌 핵심 부분\n",
    "            2. 접사 affix: 단어에 추가적인 의미를 주는 부분\n",
    "    * 어간 추출과 달리 단어의 형태가 적절히 보존\n",
    "    * 본래 단어의 품사 정보를 알아야만 정확한 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die\n",
      "watch\n",
      "have\n"
     ]
    }
   ],
   "source": [
    "# 단어의 품사(동사) 알려주어 품사의 정보 보존하면서 정확한 lemma 출력\n",
    "\n",
    "print(n.lemmatize('dies', 'v'))\n",
    "print(n.lemmatize('watched','v'))\n",
    "print(n.lemmatize('has','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 어간 추출\n",
    ": 형태학적 분석을 단순화  \n",
    ": 정해진 규칙만 보고 단어의 어미를 자르는 작업  \n",
    ": 어간 추출 후 결과는 사전에 없는 단어일 수도 있다\n",
    "    \n",
    "#### 어간 추출 알고리즘 - 포터 알고리즘 예제\n",
    "__input__ :  \n",
    "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electic']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘 규칙\n",
    "\n",
    "words = ['formalize','allowance','electicical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 포터 알고리즘 vs 랭커스터 스테머 알고리즘 - 결과 비교 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "# 랭커스터\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "l = LancasterStemmer()\n",
    "\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 4) 불용어 stopwords\n",
    "* 불용어 : 문장에 자주 등장하지만 실제 의미 분석에는 거의 필요 없음\n",
    "    * 예) I, my, me, 조사, 접미사\n",
    "    \n",
    "## 1. NLTK에서 불용어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLTK 통해 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Family is not an important thing. It's everything\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "        \n",
    "print(word_tokens)\n",
    "print(result)  # 불용어 제외된 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 한국어에서 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\" # 임의로 불용어 정의\n",
    "\n",
    "stop_words = stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "        \n",
    "# result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "  \n",
    "  \n",
    "# 5) 정규 표현식 regular expression\n",
    "\n",
    "## 1. 정규 표현식 문법과 모듈함수\n",
    "\n",
    "### (1) 정규 표현식 문법\n",
    "* . : 임의의 문자 1개\n",
    "* ? : (앞의 문자가) 0개 또는 1개. 존재할 수도, 안할수도\n",
    "* \\* : (앞의 문자가) 0개 이상\n",
    "* \\+ : (앞의 문자가) 1개 이상\n",
    "* ^ : 뒤의 문자로 문자열 시작\n",
    "* $ : 앞의 문자로 문자열 끝\n",
    "\n",
    "\n",
    "* {숫자} :숫자만큼 반복\n",
    "* {숫자1,숫자2} :숫자1 이상, 숫자2 이하 만큼 반복\n",
    "* {숫자, } :숫자 이상만큼 반복\n",
    "\n",
    "\n",
    "* [　] : 대괄호 안의 문자들 중 한 개의 문자와 매치\n",
    "    * [a-zA-Z] : 알파벳 전체 범위\n",
    "* [^문자] : 해당 문자 제외한 문자 매치\n",
    "* A __|__ B : A 또는 B\n",
    "\n",
    "\n",
    "* \\\\\\\\ : ' \\ ' 문자 자체\n",
    "* \\\\d : 모든 숫자. [0-9]\n",
    "* \\\\D : 숫자 제외한 모든 문자. [^0-9]\n",
    "* \\\\s : 공백 [\\\\t\\\\n\\\\r\\\\f\\\\v]\n",
    "* \\\\S : 공백 제외한 문자\n",
    "* \\\\w : 문자 또는 숫자 [a-zA-Z0-9]\n",
    "* \\\\W : 문자 또는 숫자 아닌 문자\n",
    "\n",
    "\n",
    "### (2) 정규표현식 모듈 함수\n",
    "* re.compile() : 정규 표현식 컴파일. 파이썬에게 전해주는 역할\n",
    "\n",
    "\n",
    "* re.search() : 문자열 전체에 대해 정규표현식과 매치되는지 검사\n",
    "\n",
    "\n",
    "* re.match() : 문자열의 처음이 정규표현식과 매치되는지 검사\n",
    "\n",
    "\n",
    "* re.split() : 정규표현식 기준으로 문자열 분리, 리스트로 리턴\n",
    "\n",
    "\n",
    "* re.findall() : 문자열에서 정규표현식과 매치되는 모든 경우의 문자열 찾아러 리스트로 리턴 (없으면 빈 리스트)\n",
    "\n",
    "\n",
    "* re.finditer() : 문자열에서 정규표현식과 매치되는 모든 경우의 문자열에 대한 아이터레이터 객체 리턴\n",
    "\n",
    "\n",
    "* re.sub() : 문자열에서 정규표현식과 일치하는 부분에 대해 다른 문자열로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 정규표현식 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "# . 기호\n",
    "\n",
    "import re\n",
    "r = re.compile(\"a.c\")\n",
    "\n",
    "print(r.search(\"kkk\"))\n",
    "print(r.search(\"abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 2), match='ac'>\n"
     ]
    }
   ],
   "source": [
    "# ? 기호\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab?c\") # b가 1개 있을수도, 없을수도\n",
    "\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"ac\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 2), match='ac'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 8), match='abbbbbbc'>\n"
     ]
    }
   ],
   "source": [
    "# * 기호\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab*c\") # b가 0개 이상~\n",
    "\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 11), match='abbbbbbbbbc'>\n"
     ]
    }
   ],
   "source": [
    "# + 기호\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab+c\") # b가 1개 이상~\n",
    "\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbbbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 1), match='a'>\n"
     ]
    }
   ],
   "source": [
    "# ^ 기호 \n",
    "\n",
    "import re\n",
    "r = re.compile(\"^a\")  # a로 시작되는 문자열만\n",
    "\n",
    "print(r.search(\"bbc\"))\n",
    "print(r.search(\"ab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# {숫자}\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab{2}c\") # b가 2개인 경우만\n",
    "\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n",
      "<re.Match object; span=(0, 10), match='abbbbbbbbc'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# {숫자1, 숫자2}\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab{2,8}c\") # b가 2개 이상, 8개 이하\n",
    "\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abbbbbbbbc\"))\n",
    "print(r.search(\"abbbbbbbbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 4), match='aabc'>\n",
      "<re.Match object; span=(0, 16), match='aaaaaaaaaaaaaabc'>\n"
     ]
    }
   ],
   "source": [
    "# {숫자,}\n",
    "\n",
    "import re\n",
    "r = re.compile(\"a{2,}bc\") # a가 2개 이상인 경우와 매치\n",
    "\n",
    "print(r.search(\"bc\"))\n",
    "print(r.search(\"aa\"))\n",
    "print(r.search(\"aabc\"))\n",
    "print(r.search(\"aaaaaaaaaaaaaabc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='b'>\n"
     ]
    }
   ],
   "source": [
    "# [ ] 기호\n",
    "\n",
    "import re\n",
    "r = re.compile(\"[abc]\") # [a-c]\n",
    "\n",
    "print(r.search(\"zzz\"))\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"aaaaaaaa\"))\n",
    "print(r.search(\"baac\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(1, 2), match='b'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 소문자에 대해서만 범위 지정\n",
    "\n",
    "import re\n",
    "r = re.compile(\"[a-z]\")\n",
    "\n",
    "print(r.search(\"AAA\"))\n",
    "print(r.search(\"Abc\"))\n",
    "print(r.search(\"111\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 1), match='d'>\n"
     ]
    }
   ],
   "source": [
    "# [^ 문자]\n",
    "\n",
    "import re\n",
    "r = re.compile(\"[^abc]\") # a 또는 b 또는 c 들어간 문자열 제외한 모든 문자열 매치\n",
    "\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"ab\"))\n",
    "print(r.search(\"b\"))\n",
    "print(r.search(\"d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정규표현식 모듈 함수 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.search()\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab.\")\n",
    "r.search(\"kkkabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "# re.match() : 문자열 시작에서 패턴이 일치해야 함\n",
    "\n",
    "import re\n",
    "r = re.compile(\"ab.\")\n",
    "\n",
    "print(r.match(\"kkkabc\"))\n",
    "print(r.match(\"abckkk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'orange', 'pear', 'grape']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.split()\n",
    "\n",
    "import re\n",
    "text = \"apple banana orange pear grape\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'orange', 'pear', 'grape']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"apple\n",
    "banana\n",
    "orange\n",
    "pear\n",
    "grape\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'orange', 'pear', 'grape']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"apple+banana+orange+pear+grape\"\n",
    "\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1111', '1234', '25']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.findall()\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"\"\"name : Do\n",
    "phone : 010 - 1111- 1234\n",
    "age : 25\n",
    "gender : female\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\",text) # 숫자만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d+\",\"list of words\") # 문자열에 숫자 없으므로 빈 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.sub()\n",
    "\n",
    "import re\n",
    "text = text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "re.sub('[^a-zA-Z]',' ',text) # 알파벳이 아닌 특수문자를 공백으로 대체하여 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 전처리 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"  \n",
    "\n",
    "# 공백 1개 이상 찾아서 분리\n",
    "re.split('\\s+', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 공백 기준으로 구분된 데이터에서 숫자만 찾기\n",
    "re.findall('\\d+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자인 행의 값만 가져오기\n",
    "## 조건 : 대문자가 4번 연속 등장하는 경우\n",
    "re.findall('[A-Z]{4}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자 & 소문자 섞여있는 이름 행의 값만 가져오기\n",
    "## 첫 문자는 대문자, 이후에 소문자 여러번 등장하는 경우\n",
    "re.findall('[A-Z][a-z]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    John    PROF     James   STUD     Mac   STUD'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영문자가 아닌 문자 모두 공백 처리\n",
    "letters_only = re.sub('[^a-zA-Z]', ' ', text)\n",
    "letters_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 정규표현식 이용한 토큰화\n",
    "__RegexpTokenizer(__ 원하는 정규 표현식 __)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\") # 문자 또는 숫자 1개 이상 (구두점 제외한 단어만 토큰화)\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True) # 1개 이상의 공백 기준으로 문장 토큰화\n",
    "# gaps=True :해당 정규 표현식을 토큰으로 나누기 위한 기준으로 사용\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "# 6) 정수 인코딩\n",
    ": 텍스트를 숫자로 바꾸기 위해 각 단어를 고유한 정수에 매핑시키는 전처리\n",
    ": 단어에 대한 빈도수 기준으로 정렬한 후 번호 인덱스 부여\n",
    "\n",
    "## 1. 정수 인코딩\n",
    "1. 단어를 빈도수 정렬\n",
    "2. 빈도수 높은 순서부터 정수 부여\n",
    "\n",
    "### (1) dictionary 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 정제 & 단어 토큰화\n",
    "vocab = {} # dictionary\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # 단어 토큰화\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word = word.lower() # 모든 단어 소문자화 = 단어 수 줄이기\n",
    "        if word not in stop_words: # 불용어 제거\n",
    "            if len(word) > 2:      # 단어 길이 2 이하이면 제거\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "    sentences.append(result)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# vocab : 중복 제거된 단어와 각 단어의 빈도수 기록\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어의 빈도수 출력\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수 단어에 낮은 정수 인덱스 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:     # 빈도수(=1) 적은 단어 제외\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 빈도의 단어만 사용\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스 5 초과 단어  제거\n",
    "\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]  # 해당 단어에 대한 인덱스 정보 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화가 진행된 sentences의 각 단어를 정수로 인코딩\n",
    "\n",
    "## OOV (Out-Of-Vocabulary) : word_to_index에 존재하지 않는 단어\n",
    "# word_to_index 에 'OOV'라는 단어 새로 추가 & 단어 집합에 없는 단어들을 'OOV'의 인덱스로 인코딩\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "\n",
    "# sentences의 모든 단어들을 정수로 인코딩\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "    \n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) counter 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences) # 단어 토큰화된 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합 생성\n",
    "# 문장의 경계 [,] 제거 & 단어들을 하나의 리스트로 집합\n",
    "words = sum(sentences, [])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# Counter : 중복된 단어 제거 & 단어 빈도수 기록\n",
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도수 출력\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 5개 빈도의 단어만 집합으로 저장\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도의 단어에 낮은 인덱스 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) NLTK 의 FreqDist 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 구분을 제거하여 입력으로 사용\n",
    "vocab = FreqDist(np.hstack(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어의 빈도수 출력\n",
    "## 단어=key, 빈도수=value로 저장\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 등장 빈도수 상위 5개 단어를 집합으로 저장\n",
    "vocab_size = 5\n",
    "\n",
    "## most_common() : 주어진 상위 빈도수의 단어만을 리턴\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수 단어에 낮은 정수 인덱스 부여. enumerate() 사용\n",
    "word_to_index = {word[0]:index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) enumerate 이해하기\n",
    ": 순서가 있는 자료형 (list, set, tupe, dictionary, string)을 입력받아 인덱스를 순차적으로 함께 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index : 0\n",
      "value : b, index : 1\n",
      "value : c, index : 2\n",
      "value : d, index : 3\n",
      "value : e, index : 4\n"
     ]
    }
   ],
   "source": [
    "test = ['a','b','c','d','e']\n",
    "\n",
    "for index, value in enumerate(test): # 입력 순서대로 0부터 인덱스 부여\n",
    "    print(\"value : {}, index : {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 케라스의 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화까지 수행된 앞의 텍스트 데이터 사용\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts(코퍼스) : 빈도수 기준으로 단어 집합 생성\n",
    "tokenizer.fit_on_texts(sentences)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# word_index : 각 단어에 부여된 인덱스 확인\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# word_counts : 각 단어 개수 몇개였는지 확인\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# text_to_sequences() : 입력 코퍼스에 대해 각 단어를 이미 정해진 인덱스로 변환\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 높은 상위 몇 개의 단어만 사용하도록 지정\n",
    "\n",
    "## 상위 5개 단어만 사용하도록 토크나이저 재정의\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # num_words는 숫자 0 부터 카운트\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)  #(상위 5개만이 아닌) 모든 단어 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts) # 모든 단어 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences)) # 1~5 순위의 단어만 보존됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# word_index, word_counts에서도 지정된 순위 단어만 남기고 싶을때\n",
    "\n",
    "tokenizer = Tokenizer() #num_words 지정 안함\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "vocab_size = 5\n",
    "\n",
    "\n",
    "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스 5 초과하는 단어 제거\n",
    "\n",
    "for w in words_frequency:\n",
    "    del tokenizer.word_index[w]  # 해당 단어에 대한 인덱스 정보 삭제\n",
    "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보 삭제\n",
    "    \n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 케라스 토크나이저는 정수 인코딩 과정에서 단어 집합에 없는 단어(OOV)를 제거  \n",
    "* Tokenizer 인자 __oov_token__ :단어 집합에 없는 단어들을 OOV로 간주하여 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합에 없는 단어들 보존\n",
    "\n",
    "## 빈도수 상위 5개 단어만을 사용\n",
    "vocab_size=5\n",
    "\n",
    "## 숫자 0 & OOV 고려해서 단어 집합 크기 +2\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token='OOV') \n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "# 'OOV'인덱스 = 1\n",
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스에 대한 정수 인코딩\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 7) 원-핫 인코딩\n",
    "* __단어 집합 vocabulary__ : 서로 다른 단어들의 집합\n",
    "    * 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓은 것\n",
    "    \n",
    "## 1. 원-핫 인코딩이란?\n",
    ": 단어의 벡터 표현 방식   \n",
    ": 단어 집합의 크기를 벡터의 차원(원-핫 벡터)으로 표현  \n",
    "1. 정수 인코딩 진행\n",
    "2. 표현하고 싶은 단어의 인덱스=1, 다른 인덱스=0 부여\n",
    "\n",
    "\n",
    "## 2. 케라스 이용한 원-핫 인코딩\n",
    "to_categorical() 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "\n",
    "# 각 단어에 대한 인코딩 결과 출력\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "\n",
    "# 정수 인코딩\n",
    "## texts_to_sequences() : 단어 집합으로만 구성된 텍스트를 정수 시퀀스로 변환\n",
    "encoded = t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 8) 단어 분리하기\n",
    "* __단어 집합__ : 기계가 훈련 단계에서 학습한 단어들의 집합\n",
    "* __OOV / UNK__ : 테스트 단계에서 등장한 기계가 모르는(배우지 못한) 단어\n",
    "    * __OOV 문제__ : 모르는 단어로 인해 문제를 제대로 풀지 못하는 상황\n",
    "    \n",
    "#### 단어 분리\n",
    "* 하나의 단어를 여러 내부 단어로 분리해서 단어를 이해하려는 전처리 작업\n",
    "* 단어 분리 토크나이저로 작업\n",
    "* 기계가 배운적 없는 단어에 대해 어느 정도 대처할 수 있도록 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) 데이터 분리\n",
    "* X, y 분리\n",
    "* 훈련 데이터, 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
